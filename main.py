import streamlit as st
import pandas as pd


contributions = pd.DataFrame (
    [
        {"Name": "Jimmy", "Contributions": "Pre-processing Data"},
        {"Name": "Carlos", "Contributions": "Visualizing Data"},
        {"Name": "Omo", "Contributions": "Pre-processing Data"},
        {"Name": "Shrenik", "Contributions": "Model Implementation and Write-Up"},
        {"Name": "Jerry", "Contributions": "Model Implementation and Write-Up"}
    ]
)
st.set_page_config(page_title='Star Classification Project', page_icon ="🌟", layout = "centered")
# Display text
st.title('Star Classification Project [(Github)](https://github.com/Skoppaka9/ML-2024-Group-58)')
st.header('CS 4641 - Fall 2024 - Group 58')

st.subheader('Introduction/Background')
st.write(":heavy_minus_sign:"*14)
st.write("Stellar classification is used by scientists to categorize stars based on their spectral characteristics (temperature, size, composition, color, brightness, etc.). By classifying these stars, scientists can better understand them by analyzing their patterns and trends, which will provide us with more knowledge and insight into the universe.")
st.write('**Literature Review:**')
st.write("""
- Armstrong et al. utilized a combination of Kohonen Self-Organizing Maps (SOMs) and Random Forest as a new method for variable star classification [1]. SOMs were used to effectively parameterize light curve shapes while Random Forest were useful for their classification schemes, especially for larger data sets [1].   

- Zhang et al. focused on improving Guide Star Catalogues (GSCs) used in star identification on star sensors in satellites [2]. Zhang et al. evaluated the performance of GSCs generated by various machine learning classification algorithms [2]. K-Nearest Neighbours (KNN) produced the best GSC [2].   

- Qi classified celestial bodies into stars, galaxies, and quasars using Decision Tree, Random Forest, and SVM models [3]. SMOTE, normalization, and data splitting were used for preprocessing [3]. Random Forest had the best computing performance and accuracy [3].

""")
st.write('**Dataset:**')
st.write("""
This [Star Dataset](https://www.kaggle.com/datasets/vinesmsuic/star-categorization-giants-and-dwarfs?select=Star9999_raw.csv) uses spectral data to distinguish whether a star is a dwarf or giant. 

- The features are visual apparent magnitude, distance between star and earth, standard error of the distance, spectral type, absolute magnitude, and the target class (0 - Dwarf, 1- Giant)
- The dataset has either 9,999/99,999 rows of raw data or 3642/39552 rows of preprocessed data. We can choose the type and how many data points we want.
""")



st.subheader('Problem Definition')
st.write(":heavy_minus_sign:"*11)
st.write("Given that technology is advancing, more data on stellar bodies will become available. Instead of manually categorizing the data everytime, we can automate the process to make it more consistent and efficient with machine learning. Our problem is a binary classification; we will classify whether a star is dwarf or giant using our ML models.")

st.subheader('Methods')
st.write(":heavy_minus_sign:"*6)
st.write("""
**Preprocessing methods:**
Our dataset was already preprocessed and pre-balanced. We still checked to see if there were any null or duplicated data in the dataset so that we could remove them, but we got zero for both. We also tried to check and remove outliers in our dataset by visualizing the features with box plots. Our attempt to remove outliers was by using the IQR method, 
         which can be seen in the code below:""")
st.image('Images/Data Check.png')
st.image('Images/Box Plot.png', caption="Box plot of the features in the dataset")
st.image('Images/Removing Outliers.png', caption="Removing outliers using the IQR method")
st.image('Images/Box Plot No Outliers.png', caption="Box Plot without outliers")
st.write("""
    We also plotted the graphs of each feature to see the correlations between them. The 'Plx' and 'e_Plx' features had what seems to be outliers and weird pattners, so we conducted PCA to see if we could reduce the dimensionality of the dataset.
        """)
st.image('Images/pairplot.png', caption="Feature Plots")
st.write("""
    Conducting PCA, we were able to see the variance of each component, and as expected, 'Vmag', 'B-V', and 'Amag', comprised most of the variance, but 'Plx' and 'e_Plx' still had about 20% of the variance together.
         """)
st.image('Images/PCA.png', caption="PCA Variance")
st.write("""
    We also plotted a heatmap of feature correlations and saw that e_Plx and Plx had a high correlations with Vmag and Amag, respectively, so we wanted to see how our model's accuracy would be affected by removing these two features.
        """)
st.image('Images/Feature Correlation.png', caption="Heatmap of Feature Correlations")
st.write("""
         Finally, we used a label encoder to encode the spectral type feature, which was a string, into a numerical value so that our model can train on it properly. We saw there was no significant differnce between using a label encoder or a one hot encoder, so we went with one hot encoder because it is used for categorical features that are not ordinal or don't have an inheret ordering.
            """)
st.image('Images/Encoding.png', caption="Label Encoding")
st.write("""
         We then dropped the 'Target Class' feature from our training data, otherwise we would be cheating, and assigned 'Target Class' values to our testing data. We then split our data into training and testing data with a 80/20 split, and we normalized the data using Standard Scaler so that we can identify true effects.
         Normalizing the data is important because it allows the model to converge faster and prevents the model from being biased towards features with larger scales, and it helped improve the performance and stability of our model. 
            """)
st.image('Images/Split-Normalize.png', caption="Splitting Data")
st.write(""" 
**ML Models:**
- We used logistic regression for our first model because it is a relatively simple model that is good for binary classification. The model assumes that the data is linearly separable and that there is a linear relationship, so it serves as a simple, interpretable model for binary classification. 
Logistic regression uses the sigmoid functions which handles the decision boundary, so that is why it is good for binary classification. Also, logistic regression is good for a dataset with fewer features and can be combined with regularization to prevent overfitting. 
Since this is a relatively simple model to implement, we decided it would be good to use as our first model because it is simple to understand and will serve as a benchmark for future models.   
""")
st.image('Images/Model.png', caption="Logistic Regression")

st.subheader('Results and Discussion')
st.write(":heavy_minus_sign:"*19)
st.write("""
In our proposal, we wanted each metric to be at least 85% for our models, and we achieved that with logistic regression. For our initial results, we tested not removing outliers, using all features, and using a label encoder. 
As can be seen in the figure, we achieved 
         <br>
        - 88.36% Accuracy
         <br>
        - 87.02% Precision
         <br>
        - 90.17% Recall
         <br>
        - 88.57% F1 Score
    Our confusion matrix can be seen below, as well, and it shows that our model did a relatively good job identifying true positive and false negatives, around 3500 in both cases. Our model performed well because we used a balanced dataset, encoded the categorical features, and split and normalized the data. Logistic regression does well with binary classification, which is why we saw good results since that is what our dataset and problem is. We didn't see amazing results because logistic regression is one of the simpler models, and we only used the default parameters.
""")
st.image("Images/Normal Results.png", caption = "Basline Results")
st.write("""
         When we tried to remove the outliers in our dataset, we saw that our accuracy, precision, recall, and F1 all decreased. There are many possible explanations for this; one being that we might have removed outliers in the wrong way, especially for our specific data set.
         Our dataset can be considered as relatively small, so removing outliers results in less data points and training data for our model to use. Additionally, the outliers may have been informatiove data points that reflect importnat variability in the data, which could have influenced the decision boundary as well.
         Outliers also represent edge cases or rare cases that are present in the real world, so removing them might make the model less generalizable to unseen data, especially if the outliers were meaningful data points.
         """)
st.image("Images/Removing Outlier Results.png", caption = "Outlier Results")
st.write("""
        Going off of the results of PCA, if we removed the 'Plx', 'e_Plx', and Spectral Type features from the dataset, we found that the our performance metrics only slightly decreased. Although there are slight changes in performance, we are decreaseing the dimensionality of our dataset by removing three features, and we are getting basically the same accuracy, precision, and recall.
        This is good because it shows that our model is not relying on these features to make predictions, and it can still make accurate predictions without them.
         """)
st.image("Images/Top 3 Components.png", caption = "PCA Results")
st.write("""
        Finally, we wanted to see the differences in performance if we used label encoding compared to one hot encoding. We found that one hot encoding had a slightly lower precision score, but slightly higher f1 and recall score.
        However, using one hot encoding makes the most sense because it is used for categorical features that are not ordinal or don't have an inheret ordering, and it is better for our model to train on. Therefore, moving forward, we will use a one-hot encoder for our data for other models to train on.
         """)
st.image("Images/One Hot Encoder.png", caption = "One Hot Encoding Results")

st.write("""
        Overall, our model performed well as every metric was above 85%, and we were able to see how different preprocessing methods affected our model's performance. We can attribute the model's success to the nature of the classification and the data pre-processing. We only used the default parameters for logistic regression, so the results may not have been the best they could've been. Some next steps include using a grid search to optimize the hyperparameters and implementing other models in the same fashion to compare which model performs the best.
         """)


st.subheader('References')
st.write(":heavy_minus_sign:"*6)
st.write("""
[1] D. G. Armstrong et al., “K2 variable catalogue – II. Machine learning classification of variable stars and eclipsing binaries in K2 fields 0–4,” Monthly Notices of the Royal Astronomical Society, vol. 456, no. 2, pp. 2260–2272, Feb. 2016, doi: https://doi.org/10.1093/mnras/stv2836. 

‌[2] J. Zhang et al., “High-Accuracy Guide Star Catalogue Generation with a Machine Learning Classification Algorithm,” Sensors, vol. 21, no. 8, p. 2647, Apr. 2021, doi: https://doi.org/10.3390/s21082647. 

[3] Z. Qi, “Stellar Classification by Machine Learning,” SHS Web of Conferences, vol. 144, p. 03006, 2022, doi: https://doi.org/10.1051/shsconf/202214403006.


""")

st.subheader('Gantt Chart')
st.image("gantt.png", caption="There is a zoom in feature when you hover over the image.")


st.subheader('Contribution Table')
st.data_editor(contributions, hide_index=True)
st.divider()
st.caption("Jimmy Vu, Shrenik Koppaka, Carlos Aponte, Omikhosen Unuigboje, Jerry Song | © 2024 Star Classification Project")


